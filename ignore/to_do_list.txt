# new stuff
1. rq.gq.pen does not work with two quantiles




#Making things nice
0. Simplify choice of algorithm. So that it has to be from a set list. Think about issue of conquer... do i really want to be checking what quantreg is using... not really. 
1. codemetar: currenlty not available on CRAN. https://devguide.ropensci.org/building.html
2. readme badges..., probably not
3. Need to update ?'rqPen-package'


# Practical stuff
1. Check accuracy of return values such as rho, penRho and nzero
2. Update choice of lambda for all penalties
3. Warm starts in cv functions maybe, by tau or previous lambda and choose which is best. 
6. Penalized rho calculations for rq.group.pen 
8. update get lambda max to depend on the penalty, and maybe make it bigger for sparsity. 
9. Cut off lambda for non-sparse sequences. 
11. Names in modelreturn when beta is only a vector and not a matrix. 
12. Smarter approach for how to handle modelreturn. For instance, it gets run many unnecessary times for the lla approaches. 
13. Maybe a better way to handle if the sum of non-convex group penalty derivatives are zero..., probably not though. Maybe if p < n run the unpenalized estimator and otherwise stop. 
14. Related to above, need to figure out a way to stop this while keeping lambda the same for all estimators. Maybe some sort of bracked based approach to estimation. Could cycle through by lambda and stop then. 
15. everything assumes tau is in some reasonable order, maybe force it to be in increasing order. 
17. ggplot version of plots. 
18. Maybe think of a way to show how the bytau plots change with lambda. 
19. Issues with ... and rq.pen, for now just got rid of it. 
20. Smarter way to handle transform_coefs
21. Figuring a stopping point for small lambdas. Non-sparse models are slower to fit. This problem is more complex with multiple tau and a.
22. Investigate a smart way to do initial estimates for beta, maybe by tau and tuning parameter. hrq_glasso does have a beta0 for initialization, so that might be a good place to start. 
23. Allow users to specify lp algorithm of interest for group penalties. 
24. Maybe add something where if non-convex estimators do not change with lambda that the lambda sequence should stop. Would need a check that the lambda value is "large"
25. Group penalty provides a totally sparse solution even if one of the group penalty factors is set to zero. 
26. Allow weights for objective function. I think this would be easy to incorporate for everything that does not depend on hqreg and even then could send those to hrqglas with a naive group penalty where each predictor is its own group.

hrqglasso

1. Maybe deal with issues where penalty factors are all zero. 

