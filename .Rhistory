n <- dim(fuel2001)[1]
index <- randomly_assign(n,5)
m1SqErr <- m2SqErr <- NULL
for(i in 1:5){
testGroup <- which(index==i)
trainData <- fuel2001[-testGroup,]
testData <- fuel2001[testGroup,]
#fit models to the training data
m3 <- lm(FuelC ~ Driver, trainData)
m4 <- lm(FuelC ~ Driver + Tax, trainData)
#Record prediction errors for each model
m1SqErr <- c(m1SqErr, (testData$FuelC - predict(m3,newdata=testData))^2)
m2SqErr <- c(m2SqErr, (testData$FuelC - predict(m4,newdata=testData))^2)
}
head(trainData)
library(Sleuth3)
head(case0501)
a1 <- aov(Lifetime~Diet,case0501)
plot(a1)
plot(a1)
a1
summary(a1)
attributes(summary(f1))
attributes(summary(a1))
summary(a1)[[1]]
summary(a1)[1,]
summary(a1)[[1]][1,]
summary(a1)[[1]][1,4]
knitr::opts_chunk$set(echo = TRUE)
library(alr4)
remove_spots <- which(row.names(fuel2001) %in% c("FL","TX","NY"))
no_tx_fl_ny <- fuel2001[-remove_spots,]
head(no_tx_fl_ny)
n <- dim(no_tx_fl_ny)[1]
m1sqer <- m2sqer <- NULL
for(i in 1:n){
train_data <- no_tx_fl_ny[-i,]
test_data <- no_tx_fl_ny[i,]
m1 <- lm(FuelC ~ Drivers, no_tx_fl_ny)
m2 <- lm(FuelC ~ Drivers + Tax, no_tx_fl_ny)
m1Pred <- predict(m1,newdata=test_data)
m2Pred <- predict(m2,newdata=test_data)
m1sqer <- c(m1sqer, (test_data$FuelC-m1Pred)^2)
m2sqer <- c(m2sqer, (test_data$FuelC-m2Pred)^2)
}
mean(m1sqer)
mean(m2sqer)
boxplot(m1sqer,m2sqer)
library(car)
head(Ornstein)
dim(Ornstein)
help(Ornstein)
library(car)
head(Ornstein)
rm(list=ls(all=TRUE))
library(alr4)
library(rqPen)
remove_spots <- which(row.names(fuel2001) %in% c("FL","TX","NY"))
no_tx_fl_ny <- fuel2001[-remove_spots,]
help("fuel2001")
log_FuelC <- log(fuel2001$FuelC,10)
n <- dim(no_tx_fl_ny)[1]
m1SqErr <- m2SqErr <- NULL
for(i in 1:n){
trainData <- no_tx_fl_ny[-i,]
testData <- no_tx_fl_ny[i,]
#fit models to the training data
m1 <- lm(FuelC ~ Drivers, trainData)
m2 <- lm(FuelC ~ Drivers + Tax, trainData)
#Record prediction errors for each model
m1SqErr <- c(m1SqErr, (testData$FuelC - predict(m1,newdata=testData))^2)
m2SqErr <- c(m2SqErr, (testData$FuelC - predict(m2,newdata=testData))^2)
}
mean(m1SqErr)
mean(m2SqErr)
log_m1SqErr <- log(m1SqErr,10)
mean(log_m1SqErr)
log_m2SqErr <- log(m2SqErr,10)
mean(log_m2SqErr)
cvData <- data.frame(error=c(m1SqErr,m2SqErr), model=c(rep("m1",n),rep("m2",n)))
boxplot(error ~ model, cvData)
summary(m1)
summary(m2)
plot(m1)
plot(m2)
set.seed(1)
n <- dim(fuel2001)[1]
index <- randomly_assign(n,5)
m1SqErr <- m2SqErr <- NULL
for(i in 1:5){
testGroup <- which(index==i)
trainData <- fuel2001[-testGroup,]
testData <- fuel2001[testGroup,]
#fit models to the training data
m3 <- lm(FuelC ~ Drivers, trainData)
m4 <- lm(FuelC ~ Drivers + Tax, trainData)
#Record prediction errors for each model
m3SqErr <- c(m3SqErr, (testData$FuelC - predict(m3,newdata=testData))^2)
m4SqErr <- c(m4SqErr, (testData$FuelC - predict(m4,newdata=testData))^2)
}
mean(m3SqErr)
mean(m4SqErr)
cvData <- data.frame(error=c(m3SqErr,m4SqErr),model=c(rep("m3",n),rep("m4",n)))
boxplot(error ~ model,cvData)
q()
a <- 4
Ornstein <- read.csv("C:/Users/b157s966/Dropbox/My PC (BUSN-1XWNDC2)/Documents/classes/bsan_scm_415/data/Ornstein.csv")
View(Ornstein)
head(Ornstein)
Ornstein <- read.csv("C:/Users/b157s966/Dropbox/My PC (BUSN-1XWNDC2)/Documents/classes/bsan_scm_415/data/Ornstein.csv")
View(Ornstein)
a1 <- arima(LakeHuron, order = c(2,0,0), xreg = time(LakeHuron) - 1920)
a2 <- arima(LakeHuron, order = c(2,0,0))
plot(a1)
library(TSA)
plot(a1)
plot(a2)
a1 <- arima(LakeHuron, order = c(2,0,0), xreg = time(LakeHuron))
a2 <- arima(LakeHuron, order = c(2,0,0))
library(TSA)
plot(a1)
plot(a2)
plot(a1,newxreg=time(LakeHuron))
plot(a2)
time(LakeHuron)
plot(a1,newxreg=seq(1973,1985))
plot(a2)
plot(a1,newxreg=seq(1973,2000))
a1 <- arima(LakeHuron, order = c(2,0,0), xreg = time(LakeHuron))
plot(a1,newxreg=seq(1973,2000))
plot(a2, n.ahead=15)
plot(a1,newxreg=seq(1973,2000))
plot(a1,newxreg=seq(1973,1985))
plot(a1,newxreg=c(time(LakeHuron),seq(1973,1985)))
plot(a2, n.ahead=15)
library(Sleuth3)
head(ex1516)
library(Sleuth3)
head(ex1516)
m1 <- lm(FirearmDeaths~Year,ex1516)
library(orcutt)
t1 <- cochrane.orcutt(m1)
a <- resid(t1)
b <- resid(m1)
a[1:5]
b[1:5]
pacf(resid(m1))
pacf(resid(m1))
pacf(resid(t1))
library(TSA)
fitted(b)[1]
fitted(t1)[1]
resid(t1)[1]
fitted(t2)[2]
fitted(t1)[2]
resid(t1)[2]
library(quantreg)
rq.fit.lasso
rq.fit.fnb
library(quantreg)
n <- 100
p <- 8
x <- matrix(rnorm(800),ncol=8)
y <- 1 + x[,1] + x[,3] - x[,8] + rnorm(n)
lam <- 20
aug_y <- c(y,rep(0,2*p))
aug_x <- rbind(x,lam*diag(p),-lam*diag(p))
aug_x <- cbind(c(rep(1,n),rep(0,2*p)),aug_x)
q1 <- rq.fit.lasso(x,y,lambda=lam)
debug(rq.fit.lasso)
q1 <- rq.fit.lasso(x,y,lambda=lam)
R
lambda
install.packages("linprog")
library(splines)
library(quantreg)
library(linprog)
library(splines)
#library(Rglpk)
p = 3
n <- 100
df <- 5
x <- rep(1,n)  #intercept modle
#x <- NULL       #no intercept model
for(i in 1:p){
x <- cbind(x,rnorm(n))
}
beta <- c(1,3,10,0)#,-8)
z1 <- runif(n)
z2 <- runif(n)
#beta <- runif(p+1) * runif(p+1,-1,1)
y <- x%*%beta + exp(z1) + sqrt(z2) + rnorm(n)
z1_basis <- bs(z1,df)
z2_basis <- bs(z2,df)
tau = .5
lambda <- 1
rho_function <- function(x,tau){
x*(tau - (x < 0))
}
pos_part <- function(x){
if(x > 0){
x
}
else{
0
}
}
scad_2_deriv <- function(beta,lambda=1,a=3.7){
sign(beta)*lambda*(1-  ( pos_part(a*lambda-abs(beta)) / ( lambda*(a-1))))*(abs(beta) > lambda)
}
scad_1_deriv <- function(beta,lambda=1,a=3.7){
sign(beta)*lambda
}
scad_deriv <- function(beta, lambda=1,a=3.7){
scad_1_deriv(beta,lambda,a) - scad_2_deriv(beta,lambda,a)
}
#trying to figure out if I programmed above right:
#also using scad2 function from additive simulations
scad_2_plot <- function(x){
plot(x,sapply(x,scad_2))
}
scad_2_deriv_est_plot <- function(x){
y <- sapply(x,scad_2)
n <- length(x)
x_prev <- c(NA,x[-n])
y_prev <- c(NA,y[-n])
deriv_est <- (y-y_prev)/(x-x_prev)
plot(x, deriv_est)
}
scad_2_deriv_plot <- function(x){
plot(x, sapply(x, scad_2_deriv))
}
scad_plot <- function(x){
plot(x, sapply(x, scad))
}
scad_deriv_plot <- function(x){
plot(x,sapply(x,scad_deriv))
}
#x <- seq(-30,30,by=.01)
#par(mfrow=c(1,2))
#scad_plot(x)
#scad_deriv_plot(x)
#scad_2_plot(x)
#scad_2_deriv_est_plot(x)
#scad_2_deriv_plot(x)
#now lets try the scad penalty where we will minimize
# \sum \tau \xi_i + (1-\tau)\zeta_i + n \lambda_n \sumj |\beta_j| - n \sumj scad_2_deriv(|\beta_j_prev|)sgn(\beta_j_prev)(\beta_j-\beta_j_prev)
##claim above is equivalent to minimizing: \sum \tau \xi_i + (1-\tau)\zeta_i + n \lambda_n \sumj |\beta_j| - n \sumj scad_2_deriv(|\beta_j_prev|)sgn(\beta_j_prev)(\beta_j)
# subject to:
# same constraints as the lasso penalty
my_scad <- rq_scad_fit(x[,-1],y,z1_basis,z2_basis,tau=tau,intercept=TRUE)
##q_scad <-  rq(y ~ x+z1_basis+z2_basis+0, method="scad", lambda=c(rep(lambda,p),rep(0,2*df)), tau=tau)
## scad penalty freezes up when you set some of the lambdas to zero, not sure what is happening here. Will check my functon compared to linear programming solution.
mm <- cbind(x,z1_basis,z2_basis)
initial_beta <- coefficients(rq(y~mm+0,tau=tau))[2:(p+1)] #don't worry about intercept term
scad_penalty <- abs( sapply(initial_beta, scad_deriv,lambda=lambda) ) #absolute value here we only want positive penalties (intuition reason) real reason: taylor expansion done on |u| term
##i will write up some more formal notes about this later... but general idea is we can replace lasso penalty with \lambda-scad deriv 2 from previous iteration
c_vec_scad <- c( c(0,scad_penalty), rep(0,2*df), c(0,scad_penalty), rep(0,2*df), tau*rep(1,n), (1-tau)*rep(1,n))
b_vec <- y
a_matrix <- cbind(x, z1_basis, z2_basis, -x, -z1_basis, -z2_basis, diag(n),-diag(n))
lp_solution_scad <- solveLP(c_vec_scad, b_vec, a_matrix, const.dir=rep("==", length(b_vec)),lpSolve=TRUE)
#coefficients(q_scad)
coefficients(my_scad)
lp_solution_scad$solution[1:(p+1+2*df)]- lp_solution_scad$solution[(p+2*df+2):(2*(p+1+2*df))]
my_resids <- resid(my_scad)[1:n]
#lp_resids <- lp_solution_scad$solution[(2*(p+2*df)+1):(2*(p+2*df)+n)]- lp_solution_scad$solution[(2*(p+2*df)+n+1):(2*(p+2*df)+2*n)]
#q_scad$rho + sum(sapply(abs(coefficients(q_scad)[1:p]), scad)) #sum(abs(coefficients(q_lasso)[-1])) #intercept sum
sum(c_vec_scad*lp_solution_scad$solution)
my_scad$rho
#my_scad_obj <- sum(sapply(my_resids, rho_function,tau=tau)) + sum(scad_penalty*abs(coefficients(my_scad)[1:p]))
#my_scad_obj
#now lets try for lasso penalty
#minimize \sumi \tau \xi + (1-\tau)\zeta_i + \lambda \sumj |\betaj|
#subject to
#y_i - x_i'\beta = \xi_i - \zeta_i (also need to take into account negative and positive betas
#\beta_j >= 0 and then have terms for negative and positive ones
##lasso seems to work ok lets not worry about it for now
# c_vec_l <- c( c(0,rep(lambda,p)), c(0,rep(lambda,p)), tau*rep(1,n), (1-tau)*rep(1,n)) #intercept c
# c_vec_l <- c( rep(lambda,p),rep(0,(2*df)),rep(lambda,p),rep(0,(2*df)), tau*rep(1,n), (1-tau)*rep(1,n))
# b_vec <- y
# a_matrix <- cbind(x, z1_basis, z2_basis, -x, -z1_basis, -z2_basis, diag(n),-diag(n))
# lp_solution_l <- solveLP(c_vec_l, b_vec, a_matrix, const.dir=rep("==", length(b_vec)),lpSolve=TRUE)
# q_lasso <- rq(y ~ x+z1_basis+z2_basis+0, method="lasso", lambda=c(rep(lambda,p),rep(0,2*df)), tau=tau) #hmm this is actually wrong, maybe check this again
# my_lasso <- rq_lasso_fit(x,y,z1_basis,z2_basis,tau=tau,intercept=FALSE)
#
# coefficients(q_lasso)
# coefficients(my_lasso)
# #lp_solution_l$solution[1:(p+1)]- lp_solution_l$solution[(p+2):(2*p+2)] #intercept model
# lp_solution_l$solution[1:(p+2*df)]- lp_solution_l$solution[(p+2*df+1):(2*(p+2*df))]
#
# q_lasso$rho + lambda*sum(abs(coefficients(q_lasso)[1:p])) #sum(abs(coefficients(q_lasso)[-1])) #intercept sum
# sum(c_vec_l*lp_solution_l$solution)
# my_lasso$rho
#standard quantile regression
# min \sumi \tau \xi + (1-\tau)\zeta_i
# subject to:
# \xi \geq 0, \zeta_i \geq 0, y_i - x_i'\beta = \xi_i + \zeta_i
#got this to work lets not worry about it anymore
###
c_vec <- c(rep(0,2*(p+1)), tau*rep(1,n), (1-tau)*rep(1,n)) #min vector (pos coeffs, neg coefs, pos resids, neg resids)
a_matrix <- cbind(x, -x, diag(n),-diag(n))
b_vec=y
lp_solution <- solveLP(c_vec, b_vec, a_matrix, const.dir=rep("==", length(b_vec)),lpSolve=TRUE)# tol = 1e-50, zero = 1e-20)
q1 <- rq(y ~ x + 0,tau=tau)
coefficients(q1)
lp_solution$solution[1:(p+1)]- lp_solution$solution[(p+2):(2*p+2)]
#now lets try for weights
#only thing that changes is c vector
##got this to work lets not worry about it anymore
##w <- runif(n)
##c_vec_w <- c(rep(0,2*(p+1)), tau*w, (1-tau)*w)
##lp_solution_w <- solveLP(c_vec_w, b_vec, a_matrix, const.dir=rep("==", length(b_vec)),lpSolve=TRUE)# tol = 1e-50, zero = 1e-20)
##qw <- rq(y ~ x + 0,weights=w,tau=tau)
##coefficients(qw)
##lp_solution_w$solution[1:(p+1)]- lp_solution_w$solution[(p+2):(2*p+2)]
c_vec_l <- c( c(0,rep(lambda,p)), c(0,rep(lambda,p)), tau*rep(1,n), (1-tau)*rep(1,n)) #intercept c
c_vec_l <- c( rep(lambda,p),rep(0,(2*df)),rep(lambda,p),rep(0,(2*df)), tau*rep(1,n), (1-tau)*rep(1,n))
b_vec <- y
a_matrix <- cbind(x, z1_basis, z2_basis, -x, -z1_basis, -z2_basis, diag(n),-diag(n))
lp_solution_l <- solveLP(c_vec_l, b_vec, a_matrix, const.dir=rep("==", length(b_vec)),lpSolve=TRUE)
q_lasso <- rq(y ~ x+z1_basis+z2_basis+0, method="lasso", lambda=c(rep(lambda,p),rep(0,2*df)), tau=tau) #hmm this is actually wrong, maybe check this again
my_lasso <- rq_lasso_fit(x,y,z1_basis,z2_basis,tau=tau,intercept=FALSE)
coefficients(q_lasso)
coefficients(my_lasso)
#lp_solution_l$solution[1:(p+1)]- lp_solution_l$solution[(p+2):(2*p+2)] #intercept model
lp_solution_l$solution[1:(p+2*df)]- lp_solution_l$solution[(p+2*df+1):(2*(p+2*df))]
q_lasso$rho + lambda*sum(abs(coefficients(q_lasso)[1:p])) #sum(abs(coefficients(q_lasso)[-1])) #intercept sum
sum(c_vec_l*lp_solution_l$solution)
my_lasso$rho
undebug(rq_lasso_fit)
c_vec_l <- c( c(0,rep(lambda,p)), c(0,rep(lambda,p)), tau*rep(1,n), (1-tau)*rep(1,n)) #intercept c
c_vec_l <- c( rep(lambda,p),rep(0,(2*df)),rep(lambda,p),rep(0,(2*df)), tau*rep(1,n), (1-tau)*rep(1,n))
b_vec <- y
a_matrix <- cbind(x, z1_basis, z2_basis, -x, -z1_basis, -z2_basis, diag(n),-diag(n))
lp_solution_l <- solveLP(c_vec_l, b_vec, a_matrix, const.dir=rep("==", length(b_vec)),lpSolve=TRUE)
q_lasso <- rq(y ~ x+z1_basis+z2_basis+0, method="lasso", lambda=c(rep(lambda,p),rep(0,2*df)), tau=tau) #hmm this is actually wrong, maybe check this again
#my_lasso <- rq_lasso_fit(x,y,z1_basis,z2_basis,tau=tau,intercept=FALSE)
coefficients(q_lasso)
coefficients(my_lasso)
#lp_solution_l$solution[1:(p+1)]- lp_solution_l$solution[(p+2):(2*p+2)] #intercept model
lp_solution_l$solution[1:(p+2*df)]- lp_solution_l$solution[(p+2*df+1):(2*(p+2*df))]
q_lasso$rho + lambda*sum(abs(coefficients(q_lasso)[1:p])) #sum(abs(coefficients(q_lasso)[-1])) #intercept sum
sum(c_vec_l*lp_solution_l$solution)
my_lasso$rho
library(rqPen)
rq.lasso.fit()
head(rq.lasso.fit)
rq.lasso.fit
rq.fit.lasso
rq.fit.br
rq.lasso.fit
#install_github("bssherwood/rqPen")
library(rqPen)
#quantreg is required for rqPen, but call directly here
#because we need the barro data set
library(quantreg)
data(barro)
y <- barro$y.net
x <- as.matrix(barro[,-1])
qbr <- rq.pen(x,y,alg="br")
qfn <- rq.pen(x,y,alg="fn")
qfn
coefficients(qfn)
qhuber <- rq.pen(x,y,alg="huber")
qhuber
coefficients(qhuber)
h1 <- hqreg(x,y)
library(hqreg)
head(hqreg)
h1 <- hqreg(x,y,method="quantile")
h1
coefficients(h1)
n <- 200
p <- 8
x <- matrix(runif(n*p),ncol=p)
y <- 1 + x[,1] + x[,8] + (1+.5*x[,3])*rnorm(100)
r1 <- rq.pen(x,y) #Lasso fit for median
# Lasso for multiple quantiles
r2 <- rq.pen(x,y,tau=c(.25,.5,.75))
# Elastic net fit for multiple quantiles
r3 <- rq.pen(x,y,penalty="ENet",a=c(0,.5,1))
library(rqPen)
n <- 200
p <- 8
x <- matrix(runif(n*p),ncol=p)
y <- 1 + x[,1] + x[,8] + (1+.5*x[,3])*rnorm(100)
r1 <- rq.pen(x,y) #Lasso fit for median
# Lasso for multiple quantiles
r2 <- rq.pen(x,y,tau=c(.25,.5,.75))
# Elastic net fit for multiple quantiles
r3 <- rq.pen(x,y,penalty="ENet",a=c(0,.5,1))
r2 <- rq.pen(x,y,tau=c(.25,.5,.75))
head(rq.pen)
library(devtools)
setwd("C:/Users/b157s966/Dropbox/My PC (BUSN-1XWNDC2)/Documents/RPackage/hrqglas")
roxygen2::roxygenise()
load_all()
setwd("C:/Users/b157s966/Dropbox/My PC (BUSN-1XWNDC2)/Documents/RPackage/rqpen")
roxygen2::roxygenise()
load_all()
help(rq.pen)
n <- 200
p <- 8
x <- matrix(runif(n*p),ncol=p)
y <- 1 + x[,1] + x[,8] + (1+.5*x[,3])*rnorm(100)
r1 <- rq.pen(x,y) #Lasso fit for median
# Lasso for multiple quantiles
r2 <- rq.pen(x,y,tau=c(.25,.5,.75))
# Elastic net fit for multiple quantiles, which must use Huber algorithm
r3 <- rq.pen(x,y,penalty="ENet",a=c(0,.5,1),alg="huber")
# First variable is not penalized
r4 <- rq.pen(x,y,penalty.factor=c(0,rep(1,7)))
tvals <- c(.1,.2,.3,.4,.5)
#Similar to penalty proposed by Belloni and Chernouzhukov.
#To be exact you would divide the tau.penalty.factor by n.
r5 <- rq.pen(x,y,tau=tvals, tau.penalty.factor=sqrt(tvals*(1-tvals)))
r1 <- rq.pen(x,y)
r1 <- rq.pen(x,y,alg="qicd")
r1 <- rq.pen(x,y,alg="QICD")
r1 <- rq.pen(x,y,alg="QICD",penalty="SCAD")
r1 <- rq.pen(x,y,alg="QICD",penalty="MCP")
r1 <- rq.pen(x,y,alg="QICD",penalty="Ridge")
r1 <- rq.pen(x,y,alg="QICD",penalty="LASSO")
r1 <- rq.pen(x,y,alg="QICD",penalty="ENet")
r1 <- rq.pen(x,y,alg="QICD",penalty="aLASSO")
library(quantreg)
help(rq)
help(rq.pen.cv)
head(barro)
help(barro)
r1 <- rq.pen(x,y,a=seq(0,1,.1),tau=seq(.1,.9,.1),penalty="huber")
r1 <- rq.pen(x,y,a=seq(0,1,.1),tau=seq(.1,.9,.1),penalty="ENet",alg="huber")
rq
plot(r1)
plot(r1,which=1)
r1 <- rq.pen(x,y,a=seq(0,1,.1),tau=seq(.1,.9,.1),penalty="ENet",alg="huber")
coefficients(r1)
r1 <- rq.pen(x,y,a=seq(0,1,.1),tau=seq(.1,.9,.1),penalty="ENet",alg="huber")
plot(r1)
coefficients(r1,tau=.1,a=0)
plot(r1,tau=.1,a=0)
debug(rq.pen)
rq.pen(x,y,penalty="Ridge")
rq.pen(x,y,penalty="Ridge")
penalty
debug(rq.enet)
lamMax
debug(rqPen:::getLamMax)
rq.pen(x,y,penalty="Ridge")
pf
npenvars
npenVars
grad_k
returnVal
library(roxygen2)
setwd("C:/Users/b157s966/Dropbox/My PC (BUSN-1XWNDC2)/Documents/RPackage/rqpen/R")
roxygen2::roxygenise()
setwd("C:/Users/b157s966/Dropbox/My PC (BUSN-1XWNDC2)/Documents/RPackage/rqpen")
roxygen2::roxygenise()
roxygen2::roxygenise()
# Chunk 1: preliminaries
options(prompt = "R> ", continue = "+  ", width = 70, useFancyQuotes = FALSE)
library("MASS")
# Chunk 2
library(rqPen)
#quantreg is required for rqPen, but call directly here
#because we need the barro data set
library(quantreg)
data(barro)
y <- barro$y.net
x <- as.matrix(barro[,-1])
qbr <- rq.pen(x,y,alg="br")
qfn <- rq.pen(x,y,alg="fn")
# Chunk 3
qhuber <- rq.pen(x,y,alg="huber")
# Chunk 4
rqe <- rq.pen(x,y,tau=.25,penalty="ENet",a=seq(0,1,.1),alg="huber")
aicm <- qic.select(rqe, method="AIC")
#BIC is the default method
bicm <- qic.select(rqe)
#following returns the coefficients of the selected model
coefficients(bicm)
#provides information about the selected model
bicm$modelsInfo
#provides IC values for all possible models
bicm$ic[1:5,1:5]
# Chunk 5
#lasso model for 9 quantiles
tvals <- seq(.1,.9,.1)
rqmt <- rq.pen(x,y,tau=tvals)
#default is to find the optimal value of
# lambda for each quantile separately
rqmt_st <- qic.select(rqmt)
# alternative option is to find one value of
# lambda best for all tau. Below code
# also provides different weights for the models
rqmt_g <- qic.select(rqmt,septau = FALSE,weights=sqrt(tvals*(1-tvals)))
# first one will have different values of lambda for each quantile
# second approach will provide the same value for lambda for all quantiles
rqmt_st$modelsInfo
rqmt_g$modelsInfo
#below command gets coefficients for the selected model optimized for each quantile
coefficients(rqmt_st)
#code is the same for a group penalty
# This is not a great use of a group penalty and only for
# example
g <- c(rep(1,4),rep(2,3),rep(3,3),rep(4,3))
rqgroup <- rq.group.pen(x,y,groups=g,tau=seq(.1,.9,.1))
rqgroup_ic <- qic.select(rqgroup)
quants <- seq(.1,.9,.1)
r1 <- rq.pen(x,y,a=seq(0,1,.1),tau=quants,penalty="ENet",alg="huber")
plot(r1,a=.1,tau=.1,logLambda = TRUE)
